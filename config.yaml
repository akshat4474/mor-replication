dataset:
  name: tinystories
  tokenizer_name: gpt2
  save_path: "./data"
  val_split: validation
  max_batches: 1000         
  max_length: 128
  batch_size: 4
  streaming: true           

model:
  hidden_dim: 256
  ff_dim: 1024
  num_experts: 2
  top_k: 1
  dropout: 0.1
  use_depth_embed: true
  max_depth: 2
  vocab_size: 50257

training:
  epochs: 1
  learning_rate: 5e-5
  weight_decay: 0.01
  use_amp: true

checkpoint_path: checkpoints/mor_test.pt
tokenizer: gpt2
